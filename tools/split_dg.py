#!/usr/bin/env python3

################################################################################
"""
Contact:    wlee9829@gmail.com 
Date:       2025_05_06 
Python:     python3.11
Script:     split_dg.py

This script is used to separate overlapped DGs generated by CRSSANT. For 
complex regions that exhibit unique duplex structures (DGs), multiple 
structural components (e.g., secondary structures, bulges, or unpaired regions)
within the same region may be masked by the current approach that groups
duplexes by percentage overlap.

Additionally, SHAPE reactivity profiles can be used to address length
variability in complex structures. Peaks correspond to regions of RNA that are 
less structured or involve transitions between secondary structure elements 
(e.g., loops or bulges). We will use reactivity thresholds to differentiate
between highly structured (low reactivity) and less structured (high 
reactivity) regions. The idea is that regions with complex structures may
show greater variability in reactivity compared to simple, stable duplexes.
Therefore, there should be a distinct bimodal/trimodal distribution in the
reactivity profile.

In summary, if a duplex has a length outside the average range of the DG
(either too short or too long) and shows a bimodal or trimodal SHAPE reactivity
distribution, we classify it as a complex or unique DG for deeper analysis.

LOGIC: 

1.  Identify candidate duplexes

    - Sort and order duplexes for independent processing 
    - Store duplex as tuple (start, end) to compare 
    - Calculate length of each DG, then average length

        AvgDG (Avg DG = sigma n, i=1 endi - starti) / n n = number of DGs, start
        and end are the positions of each DG

        Threshold distance (+/- 10% of the AvgDG to determine which DGs deviate)

        Threshold Lower = Avg - Avg * 0.1 Threshold Upper = Avg + Avg * 0.1

    - Classify based on length (if within threshold, considered normal) -
      Outside (shorter or longer) may be nested or unique 

    - For every overlap that occurs, assess the nature of the overlap. 
        
        If outlier duplex is entirely contained within another, consider it
        nested. If outlier duplex doesn’t overlap or only partially
        overlaps, it might be a distinct feature (e.g., a unique secondary
        structure or noncanonical duplex). If overlaps are detected, treat
        the outliers as unique and attempt to “split” the overlapping DGs.
        Ådjust boundaries of the duplexes: 

        If a duplex is considered a nested outlier, adjust its boundary by
        trimming the overlap and assigning it as    a distinct region. Once
        nested duplexes are split and resolved, output the final list of
        non-overlapping, detangled duplexes.

2. Define further parameters/rules

3. Integrate SHAPE data for experiments where possible

TO DO: 

    1.  Integrate into CRSSANT_birch 
    2.  Flag for setting different cutoffs 
    3.  Integrate icSHAPE data, use Gaussian Mixture Models (GMM) or kernel
        density estimation) to fit reactivity profile and determine number of
        distinct peaks.
"""
################################################################################


################################################################################
# Define version
__version__ = "2.2.0"

# Version notes
__update_notes__ = """
2.2.0
    -   Fixed logic for the splitting, working on a per-read basis

2.1.0
    -   Forced sorting and indexing of split_dg files

2.0.0
    -   Added new function to split DGs into separate files (-S, --split)
        split_dgs

1.2.0
    -   Implemented functions to write out statistics (-s, --stats)
        write_statistics_csv
        write_deviation_summary_csv

1.1.0
    -   Added threshold setting for classify_arm_deviations(-t, --threshold)

1.0.0
    -   Initial commit for function logic.
    -   extract_dgs reads bam file, returns statistics on DGs.
    -   classify_arm_deviations provides logic for categorizing arm lengths.
"""
################################################################################


################################################################################
# Import packages 
import argparse 
import csv
import math 
import os 
import pysam
import shutil
import subprocess 
import sys
from collections import defaultdict
from statistics import mean, median
################################################################################


################################################################################
# Define sub-functions
def extract_dgs(bam_file):
    """
    Extract left and right arm lengths of split reads from CIGAR string,
    grouped by the integer at the end of the 'DG' tag.
    Returns: dict {DG: {'count': int, 'left_mean': float, 'left_median': float, 
    'right_mean': float, 'right_median': float}}
    """
    bamfile = pysam.AlignmentFile(bam_file, "rb")
    dg_groups = defaultdict(lambda: {'left': [], 'right': []})

    for read in bamfile:
        try:
            # Parse DG tag
            dg_tag = read.get_tag('DG')
            dg_fields = [field.strip() for field in dg_tag.split(',')]
            if len(dg_fields) != 3: continue

            dg_name = f"{dg_fields[0]}_{dg_fields[1]}_{dg_fields[2]}"

            # Parse CIGAR
            cigar = read.cigartuples
            if cigar is None: continue

            # Identify arms based on the first N
            left_len, right_len = 0, 0
            found_N = False

            for op, length in cigar:
                if op == 0:  # M = alignment match (0)
                    if not found_N: left_len += length
                    else: right_len += length
                elif op == 3:  # N = skipped region (gap)
                    found_N = True  # Start right arm after N

            if found_N and (left_len > 0 and right_len > 0):
                dg_groups[dg_name]['left'].append(left_len)
                dg_groups[dg_name]['right'].append(right_len)

        except (KeyError, ValueError, AttributeError):
            continue

    # Compute statistics
    dg_stats = {
        dg: {
            'count': len(vals['left']),
            'left_mean': round(mean(vals['left']), 2),
            'left_median': median(vals['left']),
            'right_mean': round(mean(vals['right']), 2),
            'right_median': median(vals['right']),
        }
        for dg, vals in dg_groups.items()
        if len(vals['left']) > 0 and len(vals['right']) > 0
    }

    return dg_groups, dg_stats


def classify_arm_deviations(dg_groups, dg_stats, 
    threshold=0.25, use_mean=False):
    """
    Classify each DG read arm as high/low/normal relative to its group's mean/
    median. Returns: dict with detailed arm deviation counts.
    """
    arm_deviation_summary = {}

    for dg_name, arms in dg_groups.items():
        left_lengths = arms['left']
        right_lengths = arms['right']
        total_reads = len(left_lengths)

        if total_reads == 0: continue

        # Reference values
        left_ref = (dg_stats[dg_name]['left_mean'] if use_mean else 
            dg_stats[dg_name]['left_median'])
        right_ref = (dg_stats[dg_name]['right_mean'] if use_mean else 
            dg_stats[dg_name]['right_median'])

        summary = {
            'total': total_reads,
            'left_high': 0, 'left_low': 0, 'left_normal': 0,
            'right_high': 0, 'right_low': 0, 'right_normal': 0
        }

        for left_len, right_len in zip(left_lengths, right_lengths):

            # Left arm deviation
            left_ratio = (left_len - left_ref) / left_ref
            if left_ratio > threshold: summary['left_high'] += 1
            elif left_ratio < -threshold: summary['left_low'] += 1
            else: summary['left_normal'] += 1

            # Right arm deviation
            right_ratio = (right_len - right_ref) / right_ref
            if right_ratio > threshold: summary['right_high'] += 1
            elif right_ratio < -threshold: summary['right_low'] += 1
            else: summary['right_normal'] += 1

        arm_deviation_summary[dg_name] = summary

    return arm_deviation_summary


def write_statistics_csv(dg_stats, output_path):
    """
    Write DG statistics to CSV.
    """
    with open(output_path, mode='w', newline='') as csvfile:
        fieldnames = ['dg_name', 'total_count', 'left_mean', 'left_median', 
            'right_mean', 'right_median']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for dg_name, stats in dg_stats.items():
            writer.writerow({
                'dg_name': dg_name,
                'total_count': stats['count'],
                'left_mean': stats['left_mean'],
                'left_median': stats['left_median'],
                'right_mean': stats['right_mean'],
                'right_median': stats['right_median']
            })


def write_deviation_summary_csv(arm_summary, output_path):
    """
    Write DG arm deviation summary to CSV.
    """
    with open(output_path, mode='w', newline='') as csvfile:
        fieldnames = [
            'dg_name', 'total_count',
            'left_high', 'left_low', 'left_normal',
            'right_high', 'right_low', 'right_normal'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for dg_name, summary in arm_summary.items():
            row = {'dg_name': dg_name, 'total_count': summary['total']}
            row.update({k: summary[k] for k in fieldnames if k in summary})
            writer.writerow(row)


def split_dgs(bam_file, dg_groups, dg_stats, threshold=0.25, use_mean=False):
    """
    Splits input BAM reads into three output files:
    - normal (short-short or long-long),
    - abnormal_1 (short-long),
    - abnormal_2 (long-short).
    """
    arm_deviations = classify_arm_deviations(
        dg_groups, dg_stats, threshold=threshold, use_mean=use_mean)

    normal_dgs = set()
    abnormal_1_dgs = set()  # left short, right long
    abnormal_2_dgs = set()  # left long, right short

    def get_deviation_type(summary):
        left_counts = {
            'short': summary['left_low'],
            'long': summary['left_high'],
            'normal': summary['left_normal']
        }
        right_counts = {
            'short': summary['right_low'],
            'long': summary['right_high'],
            'normal': summary['right_normal']
        }

        # Get the most frequent type on each arm
        left = max(left_counts, key=left_counts.get)
        right = max(right_counts, key=right_counts.get)
        
        if left == 'short' and right == 'long':
            return 'abnormal_1'
        elif left == 'long' and right == 'short':
            return 'abnormal_2'
        else:
            return 'normal'  # mixed or ambiguous

    for dg_name, summary in arm_deviations.items():
        category = get_deviation_type(summary)
        if category == 'normal':
            normal_dgs.add(dg_name)
        elif category == 'abnormal_1':
            abnormal_1_dgs.add(dg_name)
        elif category == 'abnormal_2':
            abnormal_2_dgs.add(dg_name)

    bamfile = pysam.AlignmentFile(bam_file, "rb")
    base_name = os.path.splitext(os.path.basename(bam_file))[0]

    out_paths = {
        'normal': f"{base_name}_normal.unsorted.bam",
        'abnormal_1': f"{base_name}_SL.unsorted.bam",
        'abnormal_2': f"{base_name}_LS.unsorted.bam"
    }

    outs = {
        k: pysam.AlignmentFile(path, "wb", template=bamfile)
        for k, path in out_paths.items()
    }

    counters = {k: 0 for k in out_paths}
    skipped = 0

    for read in bamfile:
        try:
            dg_tag = read.get_tag('DG')
            fields = [x.strip() for x in dg_tag.split(',')]
            if len(fields) != 3:
                skipped += 1
                continue
            dg_name = f"{fields[0]}_{fields[1]}_{fields[2]}"
            if dg_name not in dg_stats:
                skipped += 1
                continue

            # Parse CIGAR to get left/right lengths
            cigar = read.cigartuples
            if cigar is None:
                skipped += 1
                continue

            left_len, right_len = 0, 0
            found_N = False
            for op, length in cigar:
                if op == 0:  # M
                    if not found_N:
                        left_len += length
                    else:
                        right_len += length
                elif op == 3:  # N
                    found_N = True

            if not found_N or left_len == 0 or right_len == 0:
                skipped += 1
                continue

            # Deviation classification using that DG's stats
            left_ref = (dg_stats[dg_name]['left_mean'] if use_mean 
                else dg_stats[dg_name]['left_median'])
            right_ref = (dg_stats[dg_name]['right_mean'] if use_mean 
                else dg_stats[dg_name]['right_median'])

            l_ratio = (left_len - left_ref) / left_ref
            r_ratio = (right_len - right_ref) / right_ref

            l_dev = ('short' if l_ratio < -threshold 
                else 'long' if l_ratio > threshold else 'normal')
            r_dev = ('short' if r_ratio < -threshold 
                else 'long' if r_ratio > threshold else 'normal')

            # Route the read
            if l_dev == r_dev and l_dev in {'short', 'long'}:
                outs['normal'].write(read)
                counters['normal'] += 1
            elif l_dev == 'short' and r_dev == 'long':
                outs['abnormal_1'].write(read)
                counters['abnormal_1'] += 1
            elif l_dev == 'long' and r_dev == 'short':
                outs['abnormal_2'].write(read)
                counters['abnormal_2'] += 1
            else:
                skipped += 1

        except Exception:
            skipped += 1

    bamfile.close()
    for out in outs.values(): out.close()

    for k, unsorted_path in out_paths.items():
        sorted_path = unsorted_path.replace('.unsorted', '')
        pysam.sort("-o", sorted_path, unsorted_path)
        pysam.index(sorted_path)
        os.remove(unsorted_path)
        print(f"[+] {k.capitalize()} DG reads: {counters[k]} -> {sorted_path}")

    if skipped:
        print(f"[i] Skipped reads (unclassified or malformed): {skipped}")


def parse_arguments():
    """
    Set up command line arguments.
    """
    parser = argparse.ArgumentParser(
        description='Process DGs to isolate nested XLRNA duplexes.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    subparsers= parser.add_subparsers(
        dest='command', 
        title='Available functions', 
        required=True)

    # check_dgs subparser
    check_parser = subparsers.add_parser('check_dgs', 
        help='Processes DG bam files generated after CRSSANT to determine\
            which DGs have nested reads.')

    check_parser.add_argument('input', 
        help='Input DG BAM file (required positional argument)')
    check_parser.add_argument('-s', '--stats', 
        action='store_true', 
        help='Print statistics for the nested arms')
    check_parser.add_argument('-t', '--threshold', 
        type=float, 
        default=0.25,
        help="Threshold percentage for calculating DG outliers")
    check_parser.add_argument('-m', '--use-mean', 
        action='store_true',
        help="Use mean-based nesting detection")

    # split_dgs subparser
    split_parser = subparsers.add_parser('split_dgs',
        help='Splits DGs with abnormal arm lengths into separate BAM files.')
    split_parser.add_argument('input', 
        help='Input DG BAM file (required positional argument)')
    split_parser.add_argument('-t', '--threshold', 
        type=float, 
        default=0.25,
        help="Threshold percentage for calculating DG outliers")
    split_parser.add_argument('-m', '--use-mean', 
        action='store_true',
        help="Use mean-based nesting detection")

    return parser.parse_args()

################################################################################


################################################################################
# Execute main

def main():
    args = parse_arguments()
    try: 
        nproc = subprocess.run("nproc", 
            shell=True, check=True, text=True, capture_output=True)
        n_threads = int(nproc.stdout.strip())

    except subprocess.CalledProcessError:
        try: n_threads = os.cpu_count()
        except Exception as e:
            print(  f"Error in retrieving CPU count: {e}. "
                    f"Defaulting to 1 thread.")
            n_threads = 1

    if args.command == 'check_dgs':
        # Extract both read data and statistics
        dg_groups, dg_stats = extract_dgs(bam_file=args.input)
        print("\n=== DG Statistics ===")
        for dg, metrics in dg_stats.items():
            print(f"DG={dg}: {metrics}")

        # Run deviation classifier
        deviations = classify_arm_deviations(dg_groups, dg_stats, 
            threshold=args.threshold, use_mean=args.use_mean)

        print(f"\n=== Arm Deviation Summary ({int(args.threshold * 100)}% "
            f"from {'median' if args.use_mean else 'mean'}) ===")
        for dg, summary in deviations.items(): print(f"DG={dg}: {summary}")

        if args.stats:
            base_name = os.path.splitext(os.path.basename(args.input))[0]
            stats_csv = f"{base_name}_dg_statistics.csv"
            deviations_csv = f"{base_name}_arm_deviations.csv"

            # Write CSVs
            write_statistics_csv(dg_stats, stats_csv)
            write_deviation_summary_csv(deviations, deviations_csv)

            print(f"\n[+] DG statistics written to: {stats_csv}")
            print(f"[+] Arm deviation summary written to: {deviations_csv}")

    elif args.command == "split_dgs":
        # Extract both read data and statistics
        dg_groups, dg_stats = extract_dgs(bam_file=args.input)
        split_dgs(bam_file=args.input, dg_groups=dg_groups, dg_stats=dg_stats, 
                  threshold=args.threshold, use_mean=args.use_mean)

if __name__ == "__main__":
    main()
################################################################################