#!/usr/bin/env python3

################################################################################
"""
Contact:    wlee9829@gmail.com 
Date:       2025_05_06 
Python:     python3.11
Script:     split_dg.py

This script is used to separate overlapped DGs generated by CRSSANT. For 
complex regions that exhibit unique duplex structures (DGs), multiple 
structural components (e.g., secondary structures, bulges, or unpaired regions)
within the same region may be masked by the current approach that groups
duplexes by percentage overlap.

Additionally, SHAPE reactivity profiles can be used to address length
variability in complex structures. Peaks correspond to regions of RNA that are 
less structured or involve transitions between secondary structure elements 
(e.g., loops or bulges). Use reactivity thresholds to differentiate
between highly structured and lesser structured regions. Regions with complex 
structures may show greater variability in reactivity compared to simple, 
stable duplexes. Therefore, there should be a distinct bimodal/trimodal
distribution in the reactivity profile.

If a duplex arm has a length outside the average range of the DG
(either too short or too long) and shows a bimodal or trimodal SHAPE reactivity
distribution, we classify it as a complex or unique DG for deeper analysis.

LOGIC: 

1.  Identify candidate duplexes

    - Sort reads by DG tag and separate each reach within for processing 
    - Store duplex arm lengths into dictionary
    - Calculate median and mean length of each DG arms

        AvgDG (Avg DG = sigma n, i=1 endi - starti) / n n = number of DGs, start
        and end are the positions of each DG

    - Classify based on length (if within threshold, considered normal) 

        Default setting is median, since it provides higher resistance to 
        outliers. Sparse abnormally long or short reads won't skew median. 
        Better for skewed or non-normal distributions (as in the case of DGs)

    - Outside (either arm shorter or longer) may be nested or unique 

        Threshold distance (+/-% of the AvgDG to determine which DGs deviate)

    - For every overlap that occurs, assess the nature of the overlap. 
        
        If outlier duplex is entirely contained within another, consider it
        nested. If outlier duplex doesn’t overlap or only partially overlaps,
        it might be a distinct feature (e.g., a unique secondary structure or 
        noncanonical duplex). If overlaps are detected, treat the outliers as 
        unique and attempt to “split” the overlapping DGs.

        Ådjust boundaries of the duplexes: 

        If a duplex is considered a nested outlier, adjust its boundary by
        trimming the overlap and assigning it as    a distinct region. Once
        nested duplexes are split and resolved, output the final list of
        non-overlapping, detangled duplexes.

2. Define further parameters/rules

3. Integrate SHAPE data for experiments where possible

TO DO: 

    1.  Integrate into CRSSANT_birch 
    2.  Flag for setting different cutoffs 
    3.  Integrate icSHAPE data, use Gaussian Mixture Models (GMM) or kernel
        density estimation) to fit reactivity profile and determine number of
        distinct peaks.
"""
################################################################################


################################################################################
# Define version
__version__ = "2.2.0"

# Version notes
__update_notes__ = """
2.3.0
    -   Added optional setting for cutoffs (-c, --cutoff) defaults 5
    -   Added flag for rewriting DGs in-place (-r, --rewrite-tags)
2.2.0
    -   Fixed logic for the splitting, working on a per-read basis
    -   Added flag for using mean (-m, --use-mean) defaults to median
2.1.0
    -   Forced sorting and indexing of split_dg files
2.0.0
    -   Added new function to split DGs into separate files
        split_dgs
1.2.0
    -   Implemented functions to write out statistics (-s, --stats)
        write_statistics_csv
        write_deviation_summary_csv
1.1.0
    -   Added threshold setting for classify_arm_deviations(-t, --threshold)
1.0.0
    -   Initial commit for function logic.
    -   extract_dgs reads bam file, returns statistics on DGs.
    -   classify_arm_deviations provides logic for categorizing arm lengths.
"""
################################################################################


################################################################################
# Import packages 
import argparse 
import csv
import math 
import os 
import pysam
import shutil
import subprocess 
import sys
from collections import defaultdict
from statistics import mean, median
################################################################################


################################################################################
# Define sub-functions
def extract_dgs(bam_file, cutoff=5):
    """
    Extract left and right arm lengths of split reads from CIGAR string,
    grouped by the integer at the end of the 'DG' tag.
    Returns: dict {DG: {'count': int, 'left_mean': float, 'left_median': float, 
    'right_mean': float, 'right_median': float}}
    """
    bamfile = pysam.AlignmentFile(bam_file, "rb")
    dg_groups = defaultdict(lambda: {'left': [], 'right': []})

    for read in bamfile:
        try:
            # Parse DG tag
            dg_tag = read.get_tag('DG')
            dg_fields = [field.strip() for field in dg_tag.split(',')]
            if len(dg_fields) != 3: 
                continue
            dg_name = f"{dg_fields[0]}_{dg_fields[1]}_{dg_fields[2]}"

            # Parse CIGAR
            cigar = read.cigartuples
            if cigar is None: 
                continue
            left_len, right_len = 0, 0
            found_N = False

            for op, length in cigar:
                if op == 0:  # M = alignment match (0)
                    if not found_N: left_len += length
                    else: right_len += length
                elif op == 3:  # N = skipped region (gap)
                    found_N = True  # Start right arm after N

            if found_N and (left_len > 0 and right_len > 0):
                dg_groups[dg_name]['left'].append(left_len)
                dg_groups[dg_name]['right'].append(right_len)

        except (KeyError, ValueError, AttributeError):
            continue

    # Compute statistics
    dg_stats = {
        dg: {
            'count': len(vals['left']),
            'left_mean': round(mean(vals['left']), 2),
            'left_median': median(vals['left']),
            'right_mean': round(mean(vals['right']), 2),
            'right_median': median(vals['right']),
        }
        for dg, vals in dg_groups.items()
        if len(vals['left']) > int(cutoff) and len(vals['right']) > int(cutoff)
    }

    return dg_groups, dg_stats


def classify_arm_deviations(dg_groups, dg_stats, 
    threshold=0.25, use_mean=False, cutoff=5):
    """
    Classify each DG read arm as high/low/normal relative to its group's mean/
    median. Returns: dict with detailed arm deviation counts.
    """
    arm_deviation_summary = {}

    for dg_name, arms in dg_groups.items():
        left_lengths = arms['left']
        right_lengths = arms['right']
        total_reads = len(arms['left'])

        # Skip DGs with insufficient read support
        if total_reads <= int(cutoff) or dg_name not in dg_stats:
            continue

        # Reference values
        left_ref = (dg_stats[dg_name]['left_mean'] if use_mean else 
            dg_stats[dg_name]['left_median'])
        right_ref = (dg_stats[dg_name]['right_mean'] if use_mean else 
            dg_stats[dg_name]['right_median'])

        summary = {
            'total': total_reads,
            'left_high': 0, 'left_low': 0, 'left_normal': 0,
            'right_high': 0, 'right_low': 0, 'right_normal': 0
        }

        for left_len, right_len in zip(left_lengths, right_lengths):

            # Left arm deviation
            left_ratio = (left_len - left_ref) / left_ref
            if left_ratio > threshold: summary['left_high'] += 1
            elif left_ratio < -threshold: summary['left_low'] += 1
            else: summary['left_normal'] += 1

            # Right arm deviation
            right_ratio = (right_len - right_ref) / right_ref
            if right_ratio > threshold: summary['right_high'] += 1
            elif right_ratio < -threshold: summary['right_low'] += 1
            else: summary['right_normal'] += 1

        arm_deviation_summary[dg_name] = summary

    return arm_deviation_summary


def write_statistics_csv(dg_stats, output_path):
    """
    Write DG statistics to CSV.
    """
    with open(output_path, mode='w', newline='') as csvfile:
        fieldnames = ['dg_name', 'total_count', 'left_mean', 'left_median', 
            'right_mean', 'right_median']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for dg_name, stats in dg_stats.items():
            writer.writerow({
                'dg_name': dg_name,
                'total_count': stats['count'],
                'left_mean': stats['left_mean'],
                'left_median': stats['left_median'],
                'right_mean': stats['right_mean'],
                'right_median': stats['right_median']
            })


def write_deviation_summary_csv(arm_summary, output_path):
    """
    Write DG arm deviation summary to CSV.
    """
    with open(output_path, mode='w', newline='') as csvfile:
        fieldnames = [
            'dg_name', 'total_count',
            'left_high', 'left_low', 'left_normal',
            'right_high', 'right_low', 'right_normal'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for dg_name, summary in arm_summary.items():
            row = {'dg_name': dg_name, 'total_count': summary['total']}
            row.update({k: summary[k] for k in fieldnames if k in summary})
            writer.writerow(row)


def parse_cigar_arms(cigar):
    """Extract left and right arm lengths based on M before and after first N."""
    left, right = 0, 0
    found_N = False
    for op, length in cigar:
        if op == 0:  # M
            if not found_N:
                left += length
            else:
                right += length
        elif op == 3:  # N
            found_N = True
    return (left, right) if found_N and left > 0 and right > 0 else (None, None)


def get_read_deviation_type(left_len, right_len, left_ref, right_ref, 
    threshold=0.25):
    left_ratio = (left_len - left_ref) / left_ref if left_ref else 0
    right_ratio = (right_len - right_ref) / right_ref if right_ref else 0

    left_type = 'short' if left_ratio < -threshold else 'long' if left_ratio > threshold else 'normal'
    right_type = 'short' if right_ratio < -threshold else 'long' if right_ratio > threshold else 'normal'

    if left_type == 'short' and right_type == 'long':
        return 'SL'
    elif left_type == 'long' and right_type == 'short':
        return 'LS'
    else:
        return 'normal'


def sort_and_index_bam(in_path, out_path):
    pysam.sort("-o", out_path, in_path)
    pysam.index(out_path)
    os.remove(in_path)
    return out_path


def split_dgs(bam_file, threshold, use_mean=False, rewrite_tags=False, 
    cutoff=5):
    base_name = os.path.splitext(os.path.basename(bam_file))[0]

    bam = pysam.AlignmentFile(bam_file, 'rb')
    if rewrite_tags:
        output_bam = pysam.AlignmentFile(f"{base_name}.rewritten.unsorted.bam",
             'wb', template=bam)
    else:
        outs = {
            'normal': pysam.AlignmentFile(f'{base_name}.normal.unsorted.bam', 
                'wb', template=bam),
            'SL': pysam.AlignmentFile(f'{base_name}.SL.unsorted.bam', 'wb', 
                template=bam),
            'LS': pysam.AlignmentFile(f'{base_name}.LS.unsorted.bam', 'wb',
                template=bam),
        }

    dg_reads = defaultdict(list)
    dg_arm_lengths = defaultdict(lambda: {'left': [], 'right': []})

    for read in bam:
        if read.is_unmapped or not read.has_tag('DG'):
            continue

        dg_tag = read.get_tag('DG')
        fields = [f.strip() for f in dg_tag.split(',')]
        if len(fields) != 3:
            continue
        dg_name = f"{fields[0]}_{fields[1]}_{fields[2]}"

        cigar = read.cigartuples
        if cigar is None:
            continue

        left_len, right_len = parse_cigar_arms(cigar)
        if left_len is None or right_len is None:
            continue

        dg_reads[dg_name].append((read, left_len, right_len))
        dg_arm_lengths[dg_name]['left'].append(left_len)
        dg_arm_lengths[dg_name]['right'].append(right_len)

    stat_func = mean if use_mean else median
    dg_stats = {
        dg: {
            'left_median': stat_func(lengths['left']),
            'right_median': stat_func(lengths['right']),
        }
        for dg, lengths in dg_arm_lengths.items()
    }

    counts = {'normal': 0, 'SL': 0, 'LS': 0}

    # Classify and write reads
    for dg, read_list in dg_reads.items():
        if len(read_list) < int(cutoff):
            for read, _, _ in read_list:
                counts['normal'] += 1
                if rewrite_tags:
                    output_bam.write(read)
                else:
                    outs['normal'].write(read)
            continue

        ref_l = dg_stats[dg]['left_median']
        ref_r = dg_stats[dg]['right_median']

        for read, l, r in read_list:
            dev_type = get_read_deviation_type(l, r, ref_l, ref_r, threshold)
            counts[dev_type] += 1
            if rewrite_tags:
                if dev_type in ('SL', 'LS'):
                    old_tag = read.get_tag('DG')
                    new_tag = f"{old_tag}-{dev_type}"
                    read.set_tag('DG', new_tag, value_type='Z')
                output_bam.write(read)
            else:
                outs[dev_type].write(read)

    bam.close()

    if rewrite_tags:
        output_bam.close()
        sort_and_index_bam(f"{base_name}.rewritten.unsorted.bam", 
            f"{base_name}.rewritten.bam")
        print(f"[All reads] -> {base_name}.rewritten.bam")
    else:
        for out in outs.values():
            out.close()
        for dev_type in ['normal', 'SL', 'LS']:
            unsorted = f"{base_name}.{dev_type}.unsorted.bam"
            final = f"{base_name}.{dev_type}.bam"
            sort_and_index_bam(unsorted, final)
            print(f"[{dev_type}] -> {final}")

    # Summary
    print(f"\nSummary for {base_name}:")
    print(f"  Normal reads:\t{counts['normal']}")
    print(f"  SL reads:\t{counts['SL']}")
    print(f"  LS reads:\t{counts['LS']}")


def parse_arguments():
    """
    Set up command line arguments.
    """
    parser = argparse.ArgumentParser(
        description='Process DGs to isolate nested XLRNA duplexes.',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    subparsers= parser.add_subparsers(
        dest='command', 
        title='Available functions', 
        required=True)

    # check_dgs subparser
    check_parser = subparsers.add_parser('check_dgs', 
        help='Processes DG bam files generated after CRSSANT to determine\
            which DGs have nested reads')

    check_parser.add_argument('input', 
        help='Input DG BAM file (required positional argument)')
    check_parser.add_argument('-c', '--cutoff', 
        type=int, 
        default=5,
        help="Cutoff number, minimum DG counts before analysis")
    check_parser.add_argument('-m', '--use-mean', 
        action='store_true',
        help="Use mean-based nesting detection")
    check_parser.add_argument('-t', '--threshold', 
        type=float, 
        default=0.25,
        help="Threshold percentage for calculating DG outliers")
    check_parser.add_argument('-s', '--stats', 
        action='store_true', 
        help='Write statistics for the nested arms to CSV files')

    # split_dgs subparser
    split_parser = subparsers.add_parser('split_dgs',
        help='Splits DGs with abnormal arm lengths into separate BAM files')
    split_parser.add_argument('input', 
        help='Input DG BAM file (required positional argument)')
    split_parser.add_argument('-c', '--cutoff', 
        type=int, 
        default=5,
        help="Cutoff number, minimum DG counts before analysis")
    split_parser.add_argument('-m', '--use-mean', 
        action='store_true',
        help="Use mean-based nesting detection")
    split_parser.add_argument('-t', '--threshold', 
        type=float, 
        default=0.25,
        help="Threshold percentage for calculating DG outliers")
    split_parser.add_argument('-r', '--rewrite-tags', 
        action='store_true',
        help="Updates DG tags of reads with deviating arm lengths in place")

    return parser.parse_args()

################################################################################


################################################################################
# Execute main

def main():
    args = parse_arguments()
    try: 
        nproc = subprocess.run("nproc", 
            shell=True, check=True, text=True, capture_output=True)
        n_threads = int(nproc.stdout.strip())

    except subprocess.CalledProcessError:
        try: n_threads = os.cpu_count()
        except Exception as e:
            print(  f"Error in retrieving CPU count: {e}. "
                    f"Defaulting to 1 thread.")
            n_threads = 1

    if args.command == 'check_dgs':
        # Extract both read data and statistics
        dg_groups, dg_stats = extract_dgs(
            bam_file=args.input, 
            cutoff=args.cutoff)
        print("\n=== DG Statistics ===")
        for dg, metrics in dg_stats.items():
            print(f"DG={dg}: {metrics}")

        # Run deviation classifier
        deviations = classify_arm_deviations(dg_groups, dg_stats, 
            threshold=args.threshold, 
            use_mean=args.use_mean, 
            cutoff=args.cutoff)

        print(f"\n=== Arm Deviation Summary ({int(args.threshold * 100)}% "
            f"from {'median' if args.use_mean else 'mean'}) ===")
        for dg, summary in deviations.items(): print(f"DG={dg}: {summary}")

        if args.stats:
            base_name = os.path.splitext(os.path.basename(args.input))[0]
            stats_csv = f"{base_name}_dg_statistics.csv"
            deviations_csv = f"{base_name}_arm_deviations.csv"

            # Write CSVs
            write_statistics_csv(dg_stats, stats_csv)
            write_deviation_summary_csv(deviations, deviations_csv)

            print(f"\n[+] DG statistics written to: {stats_csv}")
            print(f"[+] Arm deviation summary written to: {deviations_csv}")

    elif args.command == "split_dgs":
        # Extract both read data and statistics
        split_dgs(bam_file=args.input, 
            threshold=args.threshold, 
            use_mean=args.use_mean, 
            rewrite_tags=args.rewrite_tags, 
            cutoff=args.cutoff)

if __name__ == "__main__":
    main()
################################################################################